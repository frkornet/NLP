{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is based upon the notebook that can be found on Kaggle (URL: https://www.kaggle.com/desiredewaele/sentiment-analysis-on-imdb-reviews). It is used as a basis foir developing a solution that determines whether or not a review is positive or negative. This particular model is a neural network, and uses natural language processing (NLP) concepts for processing text.\n",
    "\n",
    "The notebook has been adjusted to work with the latest tensorflow (keras) library. Changes were needed to adjust it to the latest API. This was the most challenging part.\n",
    "\n",
    "The sample data sets have been replaced by the full data sets (an increase from 5k to around 24k for both the training and test data sets. Using a larger data sets increases the accuracy on the test data from 83% to 87%.\n",
    "\n",
    "Adding layers may further improve the results. This is something to test tomorrow.\n",
    "\n",
    "Frank Kornet\n",
    "12/12/2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "import scikitplot as skplt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "np.random.seed(125)\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# modeling\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklego.mixture import GMMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklego.meta import Thresholder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# items not in Bryan's original list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# garbage\n",
    "import gc; gc.enable()\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# libraries imported by the other notebook and not in Bryan's original Notebook\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet \n",
    "allEnglishWords = words.words() + [w for w in wordnet.words()]\n",
    "allEnglishWords = np.unique([x.lower() for x in allEnglishWords])\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'hood\", \"'s_gravenhage\", \"'tween\", ..., 'zythum', 'zyzomys',\n",
       "       'zyzzogeton'], dtype='<U71')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allEnglishWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Notes.docx      README.md             exercise.ipynb\r\n",
      "LICENSE               clean_test.csv\r\n",
      "Preparing_CSVs.ipynb  clean_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('clean_train.csv')\n",
    "test_df=pd.read_csv('clean_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>If I accidentally stumbled across this script ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This film, was one of my childhood favorites a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this movie just goes to show that you dont nee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>This may be one of the worst movies to ever ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>OK I caught this film halfway through, but.oh....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review  Label\n",
       "0           0  If I accidentally stumbled across this script ...      0\n",
       "1           1  This film, was one of my childhood favorites a...      1\n",
       "2           2  this movie just goes to show that you dont nee...      1\n",
       "3           3  This may be one of the worst movies to ever ma...      0\n",
       "4           4  OK I caught this film halfway through, but.oh....      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This film is a good companion to Blair Witch, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Stanwyck and Morgan are perfectly cast in what...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Coming from the \"druggie\" generation, I though...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>It's a shame that quality actors like Baldwin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Two years after this short, the last \"Our Gang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review  Label\n",
       "0           0  This film is a good companion to Blair Witch, ...      0\n",
       "1           1  Stanwyck and Morgan are perfectly cast in what...      1\n",
       "2           2  Coming from the \"druggie\" generation, I though...      1\n",
       "3           3  It's a shame that quality actors like Baldwin ...      0\n",
       "4           4  Two years after this short, the last \"Our Gang...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor Class\n",
    "\n",
    "The code below is a preprocessor class and is used to process the review text to turn words to their base form, and get rid of unwanted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/desiredewaele/sentiment-analysis-on-imdb-reviews\n",
    "class Preprocessor(object):\n",
    "    ''' Preprocess data for NLP tasks. '''\n",
    "\n",
    "    def __init__(self, alpha=True, lower=True, stemmer=True, english=False):\n",
    "        self.alpha = alpha\n",
    "        self.lower = lower\n",
    "        self.stemmer = stemmer\n",
    "        self.english = english\n",
    "        \n",
    "        self.uniqueWords = None\n",
    "        self.uniqueStems = None\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "\n",
    "        allwords = pd.DataFrame({\"word\": np.concatenate(texts.apply(lambda x: x.split()).values)})\n",
    "        self.uniqueWords = allwords.groupby([\"word\"]).size().rename(\"count\").reset_index()\n",
    "        self.uniqueWords = self.uniqueWords[self.uniqueWords[\"count\"]>1]\n",
    "        if self.stemmer:\n",
    "            self.uniqueWords[\"stem\"] = self.uniqueWords.word.apply(lambda x: PorterStemmer().stem(x)).values\n",
    "            self.uniqueWords.sort_values([\"stem\", \"count\"], inplace=True, ascending=False)\n",
    "            self.uniqueStems = self.uniqueWords.groupby(\"stem\").first()\n",
    "        \n",
    "        #if self.english: self.words[\"english\"] = np.in1d(self.words[\"mode\"], allEnglishWords)\n",
    "        print(\"Fitted.\")\n",
    "            \n",
    "    def transform(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "        if self.stemmer:\n",
    "            allwords = np.concatenate(texts.apply(lambda x: x.split()).values)\n",
    "            uniqueWords = pd.DataFrame(index=np.unique(allwords))\n",
    "            uniqueWords[\"stem\"] = pd.Series(uniqueWords.index).apply(lambda x: PorterStemmer().stem(x)).values\n",
    "            uniqueWords[\"mode\"] = uniqueWords.stem.apply(lambda x: self.uniqueStems.loc[x, \"word\"] if x in self.uniqueStems.index else \"\")\n",
    "            texts = texts.apply(lambda x: \" \".join([uniqueWords.loc[y, \"mode\"] for y in x.split()]))\n",
    "        #if self.english: texts = self.words.apply(lambda x: \" \".join([y for y in x.split() if self.words.loc[y,\"english\"]]))\n",
    "        print(\"Transformed.\")\n",
    "        return(texts)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "        self.fit(texts)\n",
    "        texts = self.transform(texts)\n",
    "        return(texts)\n",
    "    \n",
    "    def _doAlways(self, texts):\n",
    "        # Remove parts between <>'s\n",
    "        texts = texts.apply(lambda x: re.sub('<.*?>', ' ', x))\n",
    "        # Keep letters and digits only.\n",
    "        if self.alpha: texts = texts.apply(lambda x: re.sub('[^a-zA-Z0-9 ]+', ' ', x))\n",
    "        # Set everything to lower case\n",
    "        if self.lower: texts = texts.apply(lambda x: x.lower())\n",
    "        return texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Reviews\n",
    "\n",
    "Now crearte an Preprocessor object and call the preprocessor to fit and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocessor(alpha=True, lower=True, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted.\n",
      "Transformed.\n",
      "CPU times: user 48.5 s, sys: 1.76 s, total: 50.2 s\n",
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainX = preprocess.fit_transform(train_df.Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed.\n",
      "CPU times: user 41.6 s, sys: 1.46 s, total: 43.1 s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testX =  preprocess.transform(test_df.Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Preprocessed Reviews\n",
    "\n",
    "Lets look at the first review in train and test data to see how the preprocessor changed the review text. This will give us a sense of what is happening. You can see that it remove text and that it stems the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if i accidentally stumbled across this script in textual form i would read it and maybe laugh i would not however laugh at the point in the film where the director would seems to want me to laugh although i am still not altogether sure where these are i don t care if this is woody allen this writer cannot writing dialogue or at least he cannot knowingly writing dialogue then draw performance from actors capable of draw laughter from even the most  of clown for example paraphrase i m an art historians i m lo'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0][0:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If I accidentally stumbled across this script in textual form i would read it and maybe laugh. I would not, however laugh at the points in the film where the director would seem to want me to laugh. Although I am still not altogether sure where these are. I don't care if this is Woody Allen, this writer cannot write dialogue, or at least he cannot knowingly write dialogue then draw performances from actors capable of drawing laughter from even the most ticklish of clowns. For example:<br /><br />(paraphrase\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Review'][0][0:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this film is a good companion to blair witch because it does so much wrong that bw did right like bw this one pretend to be a documentary of ghostly events with each members of the team man his her own camera the sense of reality is never there however the participants are poorly written clich d characters and the events that take place are equally clich d the cat jump out of a closet falls chandelier etc also the stilted dialog and inept improved work by the overly attractive cast detract from the docu fee'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[0][0:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This film is a good companion to Blair Witch, because it does so much wrong that BW did right. Like BW, this one pretends to be a documentary of ghostly events, with each member of the team manning his/her own camera. <br /><br />The sense of reality is never there, however. The participants are poorly written clichéd characters and the events that take place are equally clichéd (the cat jumping out of a closet, falling chandelier, etc). Also the stilted dialog and inept improv work by the overly-attractive'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Review'][0][0:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46840, 3)\n"
     ]
    }
   ],
   "source": [
    "print(preprocess.uniqueWords.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it encountered different forms of the word \"disappoint\" (7 forms to be precise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18475</th>\n",
       "      <td>disappointingly</td>\n",
       "      <td>17</td>\n",
       "      <td>disappointingli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18473</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>914</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18474</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>419</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18476</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>402</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18472</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>98</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18479</th>\n",
       "      <td>disappoints</td>\n",
       "      <td>38</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18478</th>\n",
       "      <td>disappointments</td>\n",
       "      <td>22</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  count             stem\n",
       "18475  disappointingly     17  disappointingli\n",
       "18473     disappointed    914       disappoint\n",
       "18474    disappointing    419       disappoint\n",
       "18476   disappointment    402       disappoint\n",
       "18472       disappoint     98       disappoint\n",
       "18479      disappoints     38       disappoint\n",
       "18478  disappointments     22       disappoint"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess.uniqueWords[preprocess.uniqueWords.word.str.contains(\"disappoint\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It mapped these 7 forms of \"disappoint\" to 2 stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30834, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stem</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>disappoint</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointingli</th>\n",
       "      <td>disappointingly</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word  count\n",
       "stem                                   \n",
       "disappoint          disappointed    914\n",
       "disappointingli  disappointingly     17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preprocess.uniqueStems.shape)\n",
    "preprocess.uniqueStems[preprocess.uniqueStems.word.str.contains(\"disappoint\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Next, we take the preprocessed texts as input and calculate their TF-IDF's. We retain 10000 features per text. More information can be found at https://www.tfidf.com. The stopwords are calculated in the next cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'perhaps', 'next', 'without', 'ie', 'indeed', 'until', 'amoungst', 'another', 'get', 'thereby', 'couldnt', 'will', 'while', 'they', 'found', 'to', 'six', 'myself', 'besides', 'three', 'then', 'wherein', 'front', 'de', 'only', 'than', 'keep', 'first', 'am', 'whole', 'none', 'name', 'much', 'becoming', 'my', 'between', 'what', 'so', 'throughout', 'thats', 'being', 'st', 'seem', 'never', 'nine', 'onto', 'within', 'his', 'want', 'eleven', 'became', 'we', 'hundred', 'please', 'twenty', 'among', 'neither', 'beyond', 'often', 'them', 'thereafter', 'is', 'un', 'see', 'those', 'might', 'system', 'mostly', 'someone', 'back', 'who', 'under', 'as', 'which', 'eg', 'afterwards', 'toward', 'thereupon', 'ryan', 'hereupon', 'hers', 'from', 'fifty', 'our', 'above', 'call', 'ought', 'below', 'whereby', 'least', 'side', 'thence', 'nor', 'into', 'behind', 'some', 'per', 'yourself', 'itself', 'this', 'be', 'other', 'weve', 'each', 'about', 'she', 'top', 'two', 'ha', 'most', 'thru', 'of', 'amount', 'not', 'co', 'third', 'thi', 'also', 'find', 'hasnt', 'together', 'since', 'down', 'sometimes', 'although', 'susan', 'due', 'go', 'youre', 'know', 'thank', 'except', 'anywhere', 'show', 'further', 'more', 'it', 'own', 'somehow', 'were', 'whereas', 'through', 'after', 'off', 'whither', 'forty', 'several', 'less', 'put', 'should', 'wherever', 'cannot', 'can', 'before', 'theyre', 'one', 'must', 'otherwise', 'on', 'somewhere', 'however', 'nobody', 'would', 'say', 'and', 'no', 'former', 'that', 'has', 'him', 'therefore', 'have', 'thus', 'enough', 'do', 'con', 'against', 'im', 'amongst', 'interest', 'you', 'any', 'twelve', 'well', 'anyhow', 'her', 'ltd', 'fifteen', 'beforehand', 'detail', 'ourselves', 'hereby', 'herself', 'whatever', 'with', 'hence', 're', 'everyone', 'whereafter', 'sometime', 'or', 'still', 'been', 'towards', 'all', 'moreover', 'seems', 'got', 'done', 'few', 'wa', 'either', 'nothing', 'he', 'almost', 'ive', 'once', 'take', 'whose', 'meanwhile', 'us', 'others', 'whereupon', 'like', 'upon', 'themselves', 'five', 'because', 'fill', 'but', 'whom', 'for', 'full', 'four', 'up', 'both', 'always', 'anyway', 'the', 'himself', 'sixty', 'too', 'whether', 'again', 'therein', 'sincere', 'mine', 'every', 'elsewhere', 'same', 'ever', 'anyone', 'thick', 'over', 'seeming', 'are', 'yours', 'was', 'noone', 'around', 'everything', 'rather', 'your', 'along', 'something', 'ten', 'why', 'everywhere', 'else', 'via', 'during', 'whoever', 'here', 'whence', 'even', 'latter', 'describe', 'inc', 'a', 'nowhere', 'where', 'by', 'move', 'whenever', 'could', 'their', 'empty', 'yourselves', 'cant', 'bill', 'how', 'serious', 'anything', 'last', 'alone', 'beside', 'hereafter', 'give', 'me', 'mill', 'dont', 'latterly', 'becomes', 'yet', 'cry', 'when', 'if', 'very', 'bottom', 'now', 'across', 'thin', 'already', 'formerly', 'fire', 'its', 'there', 'become', 'lets', 'at', 'ask', 'nevertheless', 'ours', 'such', 'etc', 'an', 'though', 'out', 'had', 'may', 'made', 'namely', 'i', 'these', 'in', 'many', 'seemed', 'eight', 'part', 'herein'})\n"
     ]
    }
   ],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union([\"thats\",\"weve\", \"dont\",\"lets\",\"youre\",\n",
    "                                            \"im\",\"thi\",\"ha\", \"wa\",\"st\",\"ask\",\"want\",\n",
    "                                            \"like\",\"thank\",\"know\",\"susan\",\"ryan\",\n",
    "                                            \"say\",\"got\",\"ought\",\"ive\",\"theyre\"])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stop words are now removed and a vector is created. Unsure what is in the tfidf variable. And that is what we print to get a sense for what is in there after the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=10000,\n",
      "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True,\n",
      "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
      "                                      'afterwards', 'again', 'against', 'all',\n",
      "                                      'almost', 'alone', 'along', 'already',\n",
      "                                      'also', 'although', 'always', 'am',\n",
      "                                      'among', 'amongst', 'amoungst', 'amount',\n",
      "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
      "                                      'anyone', 'anything', 'anyway',\n",
      "                                      'anywhere', ...}),\n",
      "                strip_accents=None, sublinear_tf=False,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "                vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_features=10000, stop_words=stop_words) \n",
    "#, ngram_range=(1,3)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11277906 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05166133 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.09139024 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.22674979 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.12764614\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ] \n",
      "\n",
      " If I accidentally stumbled across this script in textual form i would read it and maybe laugh. I would not, however laugh at the points in the film where the director would seem to want me to laugh. Although I am still not altogether sure where these are. I don't care if this is Woody Allen, this writer cannot write dialogue, or at least he cannot knowingly write dialogue then draw performances from actors capable of drawing laughter from even the most ticklish of clowns. For example:<br /><br />(paraphrase\n",
      "CPU times: user 3.13 s, sys: 443 ms, total: 3.58 s\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainX = tfidf.fit_transform(trainX).toarray()\n",
    "print(trainX[0][0:512], '\\n\\n', train_df['Review'][0][0:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.10555656 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ] \n",
      "\n",
      " This film is a good companion to Blair Witch, because it does so much wrong that BW did right. Like BW, this one pretends to be a documentary of ghostly events, with each member of the team manning his/her own camera. <br /><br />The sense of reality is never there, however. The participants are poorly written clichéd characters and the events that take place are equally clichéd (the cat jumping out of a closet, falling chandelier, etc). Also the stilted dialog and inept improv work by the overly-attractive\n",
      "CPU times: user 2.91 s, sys: 382 ms, total: 3.29 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testX = tfidf.transform(testX).toarray()\n",
    "print(testX[0][0:512], '\\n\\n', test_df['Review'][0][0:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 10000)\n",
      "(24678, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = train_df.Label\n",
    "testY =  test_df.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 10000) (24904,)\n",
      "(24678, 10000) (24678,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape,  testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Next, we take the 10k dimensional tfidf's as input, and keep the 2000 dimensions that correlate the most with our sentiment target. The corresponding words - see below - make sense.\n",
    "\n",
    "Note to self: I guess this is using Principal Component Analysis (PCA) to reduce the number of dimensions from 10k to 2k. I do not fully understand what is happening here, and this is something I need to come back to to get a better understanding. It is currently above my skills level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.vectorize'> <numpy.vectorize object at 0x7fcb68d77810>\n"
     ]
    }
   ],
   "source": [
    "getCorrelation = np.vectorize(lambda x: pearsonr(trainX[:,x], trainY)[0])\n",
    "print(type(getCorrelation), getCorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01284482 -0.01814795  0.00904327 ...  0.01702406  0.02087278\n",
      "  0.00974081] 10000 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "correlations = getCorrelation(np.arange(trainX.shape[1]))\n",
    "print(correlations, len(correlations), type(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [3917 5371 3149 ... 9690 9885  768] 10000\n"
     ]
    }
   ],
   "source": [
    "allIndeces = np.argsort(-correlations)\n",
    "print(type(allIndeces), allIndeces, len(allIndeces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [3917 5371 3149 ... 9690 9885  768] 2000\n"
     ]
    }
   ],
   "source": [
    "bestIndeces = allIndeces[np.concatenate([np.arange(1000), np.arange(-1000, 0)])]\n",
    "print(type(bestIndeces), bestIndeces, len(bestIndeces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['great' 'love' 'excellent' 'best' 'beautiful' 'perfect' 'favorite'\n",
      " 'enjoy' 'amazing' 'performance']\n",
      "['poor' 'stupid' 'horrible' 'worse' 'terrible' 'boring' 'awful' 'waste'\n",
      " 'worst' 'bad']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(tfidf.get_feature_names())\n",
    "print(type(vocabulary))\n",
    "print(vocabulary[bestIndeces][:10])\n",
    "print(vocabulary[bestIndeces][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 10000) (24678, 10000) 9 9998\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, testX.shape, min(bestIndeces), max(bestIndeces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX[:,bestIndeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testX[:,bestIndeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 2000) (24904,)\n",
      "(24678, 2000) (24678,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We choose a very simple dense network with 6 layers, performing binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.5\n",
    "ACTIVATION = \"tanh\"\n",
    "\n",
    "model = Sequential([    \n",
    "    Dense(int(trainX.shape[1]/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(int(trainX.shape[1]/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(int(trainX.shape[1]/4), activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(100, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(20, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(5, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),    \n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 105       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 3,554,731\n",
      "Trainable params: 3,554,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(0.00005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model\n",
    "\n",
    "As the notebook says, let's go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCHSIZE = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 2000) False\n",
      "(24904,) False\n",
      "(24678, 2000) False\n",
      "(24678,) False\n"
     ]
    }
   ],
   "source": [
    "for np_arr in [trainX, trainY, testX, testY]:\n",
    "    print(np_arr.shape, None in np_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.6955 - accuracy: 0.5125 - val_loss: 0.6840 - val_accuracy: 0.6685\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.6853 - accuracy: 0.5402 - val_loss: 0.6730 - val_accuracy: 0.7692\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.6748 - accuracy: 0.5678 - val_loss: 0.6576 - val_accuracy: 0.7971\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.6580 - accuracy: 0.5981 - val_loss: 0.6349 - val_accuracy: 0.8106\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.6359 - accuracy: 0.6416 - val_loss: 0.6033 - val_accuracy: 0.8215\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 0.6070 - accuracy: 0.6856 - val_loss: 0.5657 - val_accuracy: 0.8308\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 0.5789 - accuracy: 0.7160 - val_loss: 0.5282 - val_accuracy: 0.8384\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.5491 - accuracy: 0.7550 - val_loss: 0.4955 - val_accuracy: 0.8464\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 0.5264 - accuracy: 0.7838 - val_loss: 0.4692 - val_accuracy: 0.8524\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 2s 145ms/step - loss: 0.5055 - accuracy: 0.8024 - val_loss: 0.4502 - val_accuracy: 0.8567\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.4920 - accuracy: 0.8187 - val_loss: 0.4345 - val_accuracy: 0.8608\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.4809 - accuracy: 0.8299 - val_loss: 0.4251 - val_accuracy: 0.8632\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.4716 - accuracy: 0.8399 - val_loss: 0.4152 - val_accuracy: 0.8671\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.4648 - accuracy: 0.8454 - val_loss: 0.4082 - val_accuracy: 0.8699\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.4560 - accuracy: 0.8532 - val_loss: 0.4047 - val_accuracy: 0.8696\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.4495 - accuracy: 0.8583 - val_loss: 0.3999 - val_accuracy: 0.8717\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.4452 - accuracy: 0.8625 - val_loss: 0.3956 - val_accuracy: 0.8733\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 0.4406 - accuracy: 0.8667 - val_loss: 0.3932 - val_accuracy: 0.8730\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 3s 157ms/step - loss: 0.4369 - accuracy: 0.8690 - val_loss: 0.3908 - val_accuracy: 0.8735\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.4314 - accuracy: 0.8738 - val_loss: 0.3906 - val_accuracy: 0.8713\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.4308 - accuracy: 0.8719 - val_loss: 0.3878 - val_accuracy: 0.8731\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.4266 - accuracy: 0.8757 - val_loss: 0.3869 - val_accuracy: 0.8730\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.4231 - accuracy: 0.8768 - val_loss: 0.3859 - val_accuracy: 0.8729\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.4241 - accuracy: 0.8757 - val_loss: 0.3855 - val_accuracy: 0.8720\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.4177 - accuracy: 0.8820 - val_loss: 0.3846 - val_accuracy: 0.8710\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.4166 - accuracy: 0.8820 - val_loss: 0.3839 - val_accuracy: 0.8713\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.4153 - accuracy: 0.8854 - val_loss: 0.3837 - val_accuracy: 0.8711\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 0.4131 - accuracy: 0.8834 - val_loss: 0.3837 - val_accuracy: 0.8698\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 0.4103 - accuracy: 0.8869 - val_loss: 0.3824 - val_accuracy: 0.8717\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.4108 - accuracy: 0.8864 - val_loss: 0.3838 - val_accuracy: 0.8697\n",
      "CPU times: user 9min 41s, sys: 23.8 s, total: 10min 5s\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb69138890>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(trainX, np.asarray(trainY), epochs=EPOCHS, batch_size=BATCHSIZE, \n",
    "          validation_data=(testX, np.asarray(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(EPOCHS)\n",
    "history = model.history.history\n",
    "history.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "size": 5
         },
         "name": "Test Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          0.6684901714324951,
          0.7691871523857117,
          0.7971067428588867,
          0.8106410503387451,
          0.8214604258537292,
          0.8308209776878357,
          0.8383985757827759,
          0.8464218974113464,
          0.8523786664009094,
          0.8566739559173584,
          0.8608477115631104,
          0.8631979823112488,
          0.8670880794525146,
          0.8699246048927307,
          0.8696409463882446,
          0.8716670870780945,
          0.8732879757881165,
          0.8730447888374329,
          0.8734500408172607,
          0.8713023662567139,
          0.8730853199958801,
          0.8730447888374329,
          0.8728827238082886,
          0.8719507455825806,
          0.8710187077522278,
          0.8713023662567139,
          0.871059238910675,
          0.869843602180481,
          0.8716670870780945,
          0.8697220087051392
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "size": 5
         },
         "name": "Train Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          0.6954532861709595,
          0.6853474378585815,
          0.6748396158218384,
          0.6579576730728149,
          0.635857105255127,
          0.6069501638412476,
          0.57893306016922,
          0.5491476058959961,
          0.5264207720756531,
          0.5055315494537354,
          0.49204209446907043,
          0.48087769746780396,
          0.4715590178966522,
          0.46478626132011414,
          0.45601558685302734,
          0.4495309293270111,
          0.4451896548271179,
          0.44062504172325134,
          0.43691983819007874,
          0.4314448833465576,
          0.4307558238506317,
          0.4265533983707428,
          0.4230540990829468,
          0.4241020381450653,
          0.4177435040473938,
          0.4166145324707031,
          0.41527682542800903,
          0.4131341576576233,
          0.41026192903518677,
          0.4107523560523987
         ]
        },
        {
         "marker": {
          "size": 5
         },
         "name": "Test Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          0.6839844584465027,
          0.6729921102523804,
          0.6576442122459412,
          0.6349461674690247,
          0.6033449769020081,
          0.5656733512878418,
          0.5281727313995361,
          0.49548810720443726,
          0.4692389667034149,
          0.4501890242099762,
          0.43449580669403076,
          0.4251142144203186,
          0.41519877314567566,
          0.4082144498825073,
          0.4046630859375,
          0.39986687898635864,
          0.3955940902233124,
          0.3932264447212219,
          0.39084821939468384,
          0.39063599705696106,
          0.38781216740608215,
          0.38686567544937134,
          0.3859498202800751,
          0.38554060459136963,
          0.3845871090888977,
          0.3838820159435272,
          0.3837057948112488,
          0.3837137222290039,
          0.38242948055267334,
          0.38375043869018555
         ]
        }
       ],
       "layout": {
        "font": {
         "family": "Palatino"
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Model Training Evolution"
        },
        "xaxis": {
         "dtick": 1,
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Loss"
         }
        },
        "yaxis2": {
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"dc041c8c-a28a-4d10-bd4f-633d3cc433e7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"dc041c8c-a28a-4d10-bd4f-633d3cc433e7\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'dc041c8c-a28a-4d10-bd4f-633d3cc433e7',\n",
       "                        [{\"marker\": {\"size\": 5}, \"name\": \"Test Accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"y\": [0.6684901714324951, 0.7691871523857117, 0.7971067428588867, 0.8106410503387451, 0.8214604258537292, 0.8308209776878357, 0.8383985757827759, 0.8464218974113464, 0.8523786664009094, 0.8566739559173584, 0.8608477115631104, 0.8631979823112488, 0.8670880794525146, 0.8699246048927307, 0.8696409463882446, 0.8716670870780945, 0.8732879757881165, 0.8730447888374329, 0.8734500408172607, 0.8713023662567139, 0.8730853199958801, 0.8730447888374329, 0.8728827238082886, 0.8719507455825806, 0.8710187077522278, 0.8713023662567139, 0.871059238910675, 0.869843602180481, 0.8716670870780945, 0.8697220087051392], \"yaxis\": \"y2\"}, {\"marker\": {\"size\": 5}, \"name\": \"Train Loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"y\": [0.6954532861709595, 0.6853474378585815, 0.6748396158218384, 0.6579576730728149, 0.635857105255127, 0.6069501638412476, 0.57893306016922, 0.5491476058959961, 0.5264207720756531, 0.5055315494537354, 0.49204209446907043, 0.48087769746780396, 0.4715590178966522, 0.46478626132011414, 0.45601558685302734, 0.4495309293270111, 0.4451896548271179, 0.44062504172325134, 0.43691983819007874, 0.4314448833465576, 0.4307558238506317, 0.4265533983707428, 0.4230540990829468, 0.4241020381450653, 0.4177435040473938, 0.4166145324707031, 0.41527682542800903, 0.4131341576576233, 0.41026192903518677, 0.4107523560523987]}, {\"marker\": {\"size\": 5}, \"name\": \"Test Loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"y\": [0.6839844584465027, 0.6729921102523804, 0.6576442122459412, 0.6349461674690247, 0.6033449769020081, 0.5656733512878418, 0.5281727313995361, 0.49548810720443726, 0.4692389667034149, 0.4501890242099762, 0.43449580669403076, 0.4251142144203186, 0.41519877314567566, 0.4082144498825073, 0.4046630859375, 0.39986687898635864, 0.3955940902233124, 0.3932264447212219, 0.39084821939468384, 0.39063599705696106, 0.38781216740608215, 0.38686567544937134, 0.3859498202800751, 0.38554060459136963, 0.3845871090888977, 0.3838820159435272, 0.3837057948112488, 0.3837137222290039, 0.38242948055267334, 0.38375043869018555]}],\n",
       "                        {\"font\": {\"family\": \"Palatino\"}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Model Training Evolution\"}, \"xaxis\": {\"dtick\": 1, \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"domain\": [0, 0.45], \"title\": {\"text\": \"Loss\"}}, \"yaxis2\": {\"domain\": [0.55, 1], \"title\": {\"text\": \"Accuracy\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('dc041c8c-a28a-4d10-bd4f-633d3cc433e7');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "#    go.Scatter(x=x, y=history[\"acc\"], name=\"Train Accuracy\", marker=dict(size=5), yaxis='y2'),\n",
    "    go.Scatter(x=x, y=history[\"val_accuracy\"], name=\"Test Accuracy\", marker=dict(size=5), yaxis='y2'),\n",
    "    go.Scatter(x=x, y=history[\"loss\"], name=\"Train Loss\", marker=dict(size=5)),\n",
    "    go.Scatter(x=x, y=history[\"val_loss\"], name=\"Test Loss\", marker=dict(size=5))\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title=\"Model Training Evolution\", font=dict(family='Palatino'), xaxis=dict(title='Epoch', dtick=1),\n",
    "    yaxis1=dict(title=\"Loss\", domain=[0, 0.45]), yaxis2=dict(title=\"Accuracy\", domain=[0.55, 1]),\n",
    ")\n",
    "py.iplot(go.Figure(data=data, layout=layout), show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's first centralize the probabilities and predictions with the original train and validation dataframes. Then we can print out the respective accuracies and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24899</th>\n",
       "      <td>24994</td>\n",
       "      <td>I was so looking forward to seeing this when i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333382</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24900</th>\n",
       "      <td>24996</td>\n",
       "      <td>Paul Verhoeven's predecessor to his breakout h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.794062</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24901</th>\n",
       "      <td>24997</td>\n",
       "      <td>Financially strapped Paramount pulled out all ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194794</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24902</th>\n",
       "      <td>24998</td>\n",
       "      <td>This is one of the worst movies ever made. Tri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194576</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24903</th>\n",
       "      <td>24999</td>\n",
       "      <td>FREDDY has gone from scary to funny,in this 6t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.779808</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             Review  Label  \\\n",
       "24899       24994  I was so looking forward to seeing this when i...      0   \n",
       "24900       24996  Paul Verhoeven's predecessor to his breakout h...      1   \n",
       "24901       24997  Financially strapped Paramount pulled out all ...      1   \n",
       "24902       24998  This is one of the worst movies ever made. Tri...      0   \n",
       "24903       24999  FREDDY has gone from scary to funny,in this 6t...      1   \n",
       "\n",
       "       probability  prediction  truth  \n",
       "24899     0.333382       False  False  \n",
       "24900     0.794062        True   True  \n",
       "24901     0.194794       False   True  \n",
       "24902     0.194576       False  False  \n",
       "24903     0.779808        True   True  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"probability\"] = model.predict(trainX)\n",
    "train_df[\"prediction\"] = train_df.probability-0.5>0\n",
    "train_df[\"truth\"] = train_df.Label==1\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 2s 2ms/step - loss: 0.3272 - accuracy: 0.9201\n",
      "[0.3271786570549011, 0.9200530052185059]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(trainX, np.asarray(trainY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9200530035335689\n"
     ]
    }
   ],
   "source": [
    "print((train_df.truth==train_df.prediction).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24673</th>\n",
       "      <td>24995</td>\n",
       "      <td>With its rerelease by ADV Films, I've had a ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.802117</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24674</th>\n",
       "      <td>24996</td>\n",
       "      <td>A good cast... A good idea but turns out it is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24675</th>\n",
       "      <td>24997</td>\n",
       "      <td>There seems to be a whole sub genre of cheap, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195075</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24676</th>\n",
       "      <td>24998</td>\n",
       "      <td>The concept: show 4 families of diverse ethnic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.199903</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24677</th>\n",
       "      <td>24999</td>\n",
       "      <td>I mean let's face it, all you have to do in mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             Review  Label  \\\n",
       "24673       24995  With its rerelease by ADV Films, I've had a ch...      1   \n",
       "24674       24996  A good cast... A good idea but turns out it is...      0   \n",
       "24675       24997  There seems to be a whole sub genre of cheap, ...      0   \n",
       "24676       24998  The concept: show 4 families of diverse ethnic...      0   \n",
       "24677       24999  I mean let's face it, all you have to do in mo...      1   \n",
       "\n",
       "       probability  prediction  truth  \n",
       "24673     0.802117        True   True  \n",
       "24674     0.194416       False  False  \n",
       "24675     0.195075       False  False  \n",
       "24676     0.199903       False  False  \n",
       "24677     0.802351        True   True  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"probability\"] = model.predict(testX)\n",
    "test_df[\"prediction\"] = test_df.probability-0.5>0\n",
    "test_df[\"truth\"] = test_df.Label==1\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772/772 [==============================] - 2s 2ms/step - loss: 0.3838 - accuracy: 0.8697\n",
      "[0.3837502896785736, 0.8697220087051392]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(testX, np.array(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8697220196126104\n"
     ]
    }
   ],
   "source": [
    "print((test_df.truth==test_df.prediction).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Error analysis gives us great insight in the way the model is making its errors. Often, it shows data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>truth</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11496</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>936</td>\n",
       "      <td>11417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "truth       False  True \n",
       "prediction              \n",
       "False       11496   1055\n",
       "True          936  11417"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCross = train_df.groupby([\"prediction\", \"truth\"]).size().unstack()\n",
    "trainCross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>truth</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>10862</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1404</td>\n",
       "      <td>10601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "truth       False  True \n",
       "prediction              \n",
       "False       10862   1811\n",
       "True         1404  10601"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCross = test_df.groupby([\"prediction\", \"truth\"]).size().unstack()\n",
    "testCross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10601 true positives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13486</th>\n",
       "      <td>13602</td>\n",
       "      <td>I saw this tonight with moderate expectations ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.805829</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10596</th>\n",
       "      <td>10679</td>\n",
       "      <td>In Paris, a few months before the Nazi invasio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.805815</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15204</th>\n",
       "      <td>15352</td>\n",
       "      <td>This sad romance is untellable because the dir...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.805808</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             Review  Label  \\\n",
       "13486       13602  I saw this tonight with moderate expectations ...      1   \n",
       "10596       10679  In Paris, a few months before the Nazi invasio...      1   \n",
       "15204       15352  This sad romance is untellable because the dir...      1   \n",
       "\n",
       "       probability  prediction  truth  \n",
       "13486     0.805829        True   True  \n",
       "10596     0.805815        True   True  \n",
       "15204     0.805808        True   True  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truepositives = test_df[(test_df.truth==True)&(test_df.truth==test_df.prediction)]\n",
    "print(len(truepositives), \"true positives.\")\n",
    "truepositives.sort_values(\"probability\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10862 true negatives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8274</th>\n",
       "      <td>8331</td>\n",
       "      <td>I was truly looking forward to this title. It ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194324</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21050</th>\n",
       "      <td>21303</td>\n",
       "      <td>Movie Title - Tart&lt;br /&gt;&lt;br /&gt;Date of review -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194332</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13596</th>\n",
       "      <td>13713</td>\n",
       "      <td>Do not rent this movie. I ended up buying the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194335</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             Review  Label  \\\n",
       "8274         8331  I was truly looking forward to this title. It ...      0   \n",
       "21050       21303  Movie Title - Tart<br /><br />Date of review -...      0   \n",
       "13596       13713  Do not rent this movie. I ended up buying the ...      0   \n",
       "\n",
       "       probability  prediction  truth  \n",
       "8274      0.194324       False  False  \n",
       "21050     0.194332       False  False  \n",
       "13596     0.194335       False  False  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truenegatives = test_df[(test_df.truth==False)&(test_df.truth==test_df.prediction)]\n",
    "print(len(truenegatives), \"true negatives.\")\n",
    "truenegatives.sort_values(\"probability\", ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1811 false positives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13782</th>\n",
       "      <td>13900</td>\n",
       "      <td>Yes, indeed we have a winner- a winner in best...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194382</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24043</th>\n",
       "      <td>24354</td>\n",
       "      <td>Okay, I'll admit that if I didn't have kids, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194388</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>3762</td>\n",
       "      <td>I've waited 9 years to watch this film, simply...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194391</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             Review  Label  \\\n",
       "13782       13900  Yes, indeed we have a winner- a winner in best...      1   \n",
       "24043       24354  Okay, I'll admit that if I didn't have kids, I...      1   \n",
       "3732         3762  I've waited 9 years to watch this film, simply...      1   \n",
       "\n",
       "       probability  prediction  truth  \n",
       "13782     0.194382       False   True  \n",
       "24043     0.194388       False   True  \n",
       "3732      0.194391       False   True  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falsepositives = test_df[(test_df.truth==True)&(test_df.truth!=test_df.prediction)]\n",
    "print(len(falsepositives), \"false positives.\")\n",
    "falsepositives.sort_values(\"probability\", ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1404 false negatives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13405</th>\n",
       "      <td>13520</td>\n",
       "      <td>This is definitely one of the best Kung fu mov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.805789</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17257</th>\n",
       "      <td>17431</td>\n",
       "      <td>Victor Jory never became a major star. He is b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.805777</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10419</th>\n",
       "      <td>10502</td>\n",
       "      <td>This movie was pure genius. John Waters is bri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.805770</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             Review  Label  \\\n",
       "13405       13520  This is definitely one of the best Kung fu mov...      0   \n",
       "17257       17431  Victor Jory never became a major star. He is b...      0   \n",
       "10419       10502  This movie was pure genius. John Waters is bri...      0   \n",
       "\n",
       "       probability  prediction  truth  \n",
       "13405     0.805789        True  False  \n",
       "17257     0.805777        True  False  \n",
       "10419     0.805770        True  False  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falsenegatives = test_df[(test_df.truth==False)&(test_df.truth!=test_df.prediction)]\n",
    "print(len(falsenegatives), \"false negatives.\")\n",
    "falsenegatives.sort_values(\"probability\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the review that got predicted as positive most certainly - while being labeled as negative. However, we can easily recognize it as a poorly labeled sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "A group of heirs to a mysterious old mansion find out that they have to live in it as part of a clause in the will or be disinherited, but they soon find out of its history of everybody whom had lived there before them having either died in weird accidents or having had killed each other.<br /><br />You've seen it all before, and this one is too low-budget and slow paced to be scary, and doesn't have any real surprises in the climax. No special effects or gore to speak of, in fact the only really amusing thing about the whole film is the quality of the English dubbing, which at times is as bad as a cheap martial arts movie.<br /><br />3 out of 10, pretty low in the pecking order of 80's haunted house movies."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(test_df.loc[10502].Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The stage star Grace Hayes stars in this obscure little film. After years of being on the stage, she is going to visit a small college town to check up on her son who is being raised by his grandfather. The kid doesn't know who his mother is and when she sees him in a local malt shop, he's a boorish jerk. Part of the problem is that he is a college clown the other part is that he's really spoiled.<br /><br />Interestingly, it turns out that Mom is really quite wealthy and has been not only funding her son's life but is a hefty contributor to the college. So, she has the idea of forcing the young man to mature. She talks to the man who's raising Peter and has his allowance cut off completely--hoping he'll rise to the occasion. It also turns out that she puts the screws to the school because she thinks all these kids need to stop playing around and be more responsible.<br /><br />Now here's where it gets pretty dumb. To show that they know the value of hard work, she hires a bunch of them to sing and dance at the malt shop she just purchased. Talk about contrived!! What happens next is like a long and not particularly good talent show or perhaps a poor man's version of a Judy Garland/Mickey Rooney musical! I'd suggest the kids try to do something else to earn money...or perhaps sing and dance until people pay them to stop! I know I would pay them.<br /><br />Considering that the talent is far from talented, Peter is a bad actor and his change from lout to responsible adult is almost instantaneous, the whole thing is a bit hard to take. Not a very good film by any sane standard, this is an obscure film that should have stayed obscure! <br /><br />By the way, it is interesting that Ms. Hayes' son in the film, Peter, is actually her real life son. The only problem with that is that Peter Lind Hayes is a truly awful actor. He's not handsome enough to be a leading man and he comes off as either dull and uninteresting or downright obnoxious. In particular, all the stupid impressions he does are really bad and he has the charisma of the gelatin that you scrape off Spam. Despite this terrible performance, he did go on to have a reasonably successful acting career. Who would have believed it if you'd seen him in ZIS BOOM BAH--because here, he's about as welcome as the Bubonic Plague!!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(test_df.loc[23071].Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Application\n",
    "\n",
    "To use this model, we would store the model, along with the preprocessing vectorizers, and run the unseen texts through following pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = pd.Series(\"this movie very good. I liked it a lot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed.\n"
     ]
    }
   ],
   "source": [
    "unseen = preprocess.transform(unseen)       # Text preprocessing\n",
    "unseen = tfidf.transform(unseen).toarray()  # Feature engineering\n",
    "unseen = unseen[:,bestIndeces]              # Feature selection\n",
    "probability = model.predict(unseen)[0,0]  # Network feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5799436\n",
      "Positive!\n"
     ]
    }
   ],
   "source": [
    "print(probability)\n",
    "print(\"Positive!\") if probability > 0.5 else print(\"Negative!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 6) (24678, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Review', 'Label', 'probability', 'prediction', 'truth'], dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Review', 'Label', 'probability', 'prediction', 'truth'], dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
